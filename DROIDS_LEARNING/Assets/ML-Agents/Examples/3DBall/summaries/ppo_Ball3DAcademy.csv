Steps,Policy/Entropy,Policy/Learning Rate,Policy/Extrinsic Value Estimate,Policy/Extrinsic Reward,Environment/Cumulative Reward,Environment/Episode Length,Losses/Value Loss,Losses/Policy Loss
1000,1.4189444,0.00029700302,0.24663702,1.1259100295491595,1.1259100959183572,22.235546038543898,0.30169243,0.027972024
2000,1.4193707,0.00029100303,0.26283392,1.1934923693828625,1.1934924404946945,22.932754880694144,0.18105212,0.02621119
3000,1.4187359,0.000285003,0.24797966,1.2294247337674673,1.2294248119820799,23.29424778761062,0.14806743,0.023189511
4000,1.4178907,0.00027900303,0.29730338,1.3799999481088976,1.3800000354647637,24.8,0.16677004,0.019237673
5000,1.4167544,0.00027300304,0.3180247,1.5427499358355998,1.5427500378899277,26.425,0.18870917,0.025579583
6000,1.416094,0.00026700302,0.38274297,1.677864502184093,1.6778646232366252,27.7734375,0.21986045,0.017923972

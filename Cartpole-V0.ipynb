{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole-v0 \n",
    "## Méthode 1 Deep Learning avec TensorFlow\n",
    "(https://pythonprogramming.net/openai-cartpole-neural-network-example-machine-learning-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### les packages à importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\Anaconda3\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\Anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\Anaconda3\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alexandra\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pour avoir le jeu\n",
    "import gym\n",
    "# pour que l'agent agisse de manière aléatoire au début \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tflearn\n",
    "#pour construire le réseau de neurones\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected \n",
    "#pour la dernière couche\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "\n",
    "# pour voir ce que random a fait\n",
    "from statistics import mean, median\n",
    "#\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexandra\\gym\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "LR = 1e-3\n",
    "\n",
    "#pour lancer le jeu\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "\n",
    "#paramètres pour le jeu\n",
    "goal_steps = 200\n",
    "score_requirement = 70 \n",
    "initial_games = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour lancer le jeu de manière aléatoire au début\n",
    "\n",
    "def some_random_games_first():\n",
    "    # Each of these is its own game.\n",
    "    for episode in range(25):\n",
    "        env.reset()\n",
    "        # this is each frame, up to 200...but we wont make it that far.\n",
    "        for t in range(200):\n",
    "            # This will display the environment\n",
    "            # Only display if you really want to see it.\n",
    "            # Takes much longer to display it.\n",
    "            env.render()\n",
    "            \n",
    "            # This will just create a sample action in any environment.\n",
    "            # In this environment, the action can be 0 or 1, which is left or right\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "            # this executes the environment with an action, \n",
    "            # and returns the observation of the environment, \n",
    "            # the reward, if the env is over, and other info.\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "some_random_games_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentaion open ai\n",
    "Observations\n",
    "\n",
    "If we ever want to do better than take random actions at each step, it’d probably be good to actually know what our actions are doing to the environment.\n",
    "\n",
    "The environment’s step function returns exactly what we need. In fact, step returns four values. These are:\n",
    "\n",
    "    # observation (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "    # reward (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "    # done (boolean): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "    # info (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "This is just an implementation of the classic “agent-environment loop”. Each timestep, the agent chooses an action, and the environment returns an observation and a reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour creer un training data\n",
    "#to generate trainning samples\n",
    "\n",
    "def initial_population():\n",
    "    \n",
    "    # data that we are interested in trainning on\n",
    "    #(observation and move made)\n",
    "    # moves are random BUT only append IF score > 50\n",
    "    training_data = []\n",
    "    scores = []\n",
    "    accepted_scores = []\n",
    "    \n",
    "    # iterate through however many games we want:\n",
    "    # \"_\" => For ignoring the specific values\n",
    "    for _ in range(initial_games):\n",
    "        score = 0\n",
    "        # to store all the movements bc we don't know \n",
    "        #before the end of the game if we beat the score\n",
    "        game_memory = []\n",
    "        # previous observation that we saw\n",
    "        prev_observation = []\n",
    "        \n",
    "        # THE GAME for each frame in 200 (each _ => one game)\n",
    "        for _ in range(goal_steps):\n",
    "            # choose random action (0 or 1)\n",
    "            action = random.randrange(0,2)\n",
    "            # do it!\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            #ensemble équivalent à action = env.action_space.sample()\n",
    "            \n",
    "            # notice that the observation is returned FROM the action\n",
    "            # so we'll store the previous observation here, pairing\n",
    "            # the prev observation to the action we'll take.\n",
    "            #observation generated after the action\n",
    "            if len(prev_observation) > 0 :\n",
    "                game_memory.append([prev_observation, action])\n",
    "            prev_observation = observation\n",
    "            score+=reward\n",
    "            \n",
    "            #Pq?\n",
    "            if done: break\n",
    "        \n",
    "        #STOCKAGE DES BONNES PARTIES\n",
    "        # IF our score is higher than our threshold, we'd like to save\n",
    "        # every move we made\n",
    "        # NOTE the reinforcement methodology here. \n",
    "        # all we're doing is reinforcing the score, we're not trying \n",
    "        # to influence the machine in any way as to HOW that score is \n",
    "        # reached.\n",
    "        if score >= score_requirement:\n",
    "            accepted_scores.append(score)\n",
    "            for data in game_memory:\n",
    "                # convert to one-hot (this is the output layer for our neural network)\n",
    "                if data[1] == 1:\n",
    "                    output = [0,1]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1,0]\n",
    "                    \n",
    "                # saving our training data\n",
    "                training_data.append([data[0], output])\n",
    "\n",
    "        # reset env to play again\n",
    "        env.reset()\n",
    "        # save overall scores\n",
    "        scores.append(score)\n",
    "    \n",
    "    # just in case you wanted to reference later\n",
    "    training_data_save = np.array(training_data)\n",
    "    np.save('saved.npy',training_data_save)\n",
    "    \n",
    "    # some stats here, to further illustrate the neural network magic!\n",
    "    print('Average accepted score:',mean(accepted_scores))\n",
    "    print('Median score for accepted scores:',median(accepted_scores))\n",
    "    print(Counter(accepted_scores))\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(input_size):\n",
    "    network = input_data(shape=[None, input_size, 1], name='input')\n",
    "    #input size here is 4\n",
    "\n",
    "    network = fully_connected(network, 128, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "    # keep rate = 0;8\n",
    "\n",
    "    network = fully_connected(network, 256, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "\n",
    "    network = fully_connected(network, 512, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "\n",
    "    network = fully_connected(network, 256, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "\n",
    "    network = fully_connected(network, 128, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "    #5 layers\n",
    "    \n",
    "    #last layer, 2 are the moves we can make\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "    model = tflearn.DNN(network, tensorboard_dir='log')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(training_data, model=False):\n",
    "    #if we saved a model model = model_that_we_saved\n",
    "    \n",
    "    X = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
    "    y = [i[1] for i in training_data]\n",
    "\n",
    "    if not model:\n",
    "        model = neural_network_model(input_size = len(X[0]))\n",
    "    \n",
    "    model.fit({'input': X}, {'targets': y}, n_epoch=5, snapshot_step=500, show_metric=True, run_id='openai_learning')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accepted score: 82.55263157894737\n",
      "Median score for accepted scores: 78.5\n",
      "Counter({82.0: 7, 70.0: 7, 73.0: 6, 74.0: 5, 75.0: 4, 77.0: 4, 71.0: 4, 86.0: 3, 95.0: 3, 80.0: 3, 78.0: 3, 85.0: 3, 72.0: 3, 93.0: 2, 92.0: 2, 90.0: 2, 76.0: 2, 105.0: 2, 79.0: 2, 108.0: 2, 99.0: 1, 87.0: 1, 83.0: 1, 89.0: 1, 101.0: 1, 160.0: 1, 117.0: 1})\n"
     ]
    }
   ],
   "source": [
    "training_data = initial_population()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.65698\u001b[0m\u001b[0m | time: 1.108s\n",
      "| Adam | epoch: 005 | loss: 0.65698 - acc: 0.6334 -- iter: 6144/6198\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.66130\u001b[0m\u001b[0m | time: 1.118s\n",
      "| Adam | epoch: 005 | loss: 0.66130 - acc: 0.6216 -- iter: 6198/6198\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model = train_model(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 200.0\n",
      "choice 1:0.5005  choice 0:0.4995\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "choices = []\n",
    "for each_game in range(10):\n",
    "    score = 0\n",
    "    game_memory = []\n",
    "    prev_obs = []\n",
    "    env.reset()\n",
    "    for _ in range(goal_steps):\n",
    "        env.render()\n",
    "\n",
    "        if len(prev_obs)==0:\n",
    "            action = random.randrange(0,2)\n",
    "        else:\n",
    "            action = np.argmax(model.predict(prev_obs.reshape(-1,len(prev_obs),1))[0])\n",
    "\n",
    "        choices.append(action)\n",
    "                \n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        prev_obs = new_observation\n",
    "        game_memory.append([new_observation, action])\n",
    "        score+=reward\n",
    "        if done: break\n",
    "\n",
    "    scores.append(score)\n",
    "\n",
    "print('Average Score:',sum(scores)/len(scores))\n",
    "print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))\n",
    "print(score_requirement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé méthode 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) Lancer le jeu avec des mouvements aléatoire\n",
    "2) Stocker les données atteignant un certain seuil uniquement\n",
    "3) transformer les mouvements en one-hot --> faire un training_data\n",
    "4) Construire un réseau de neurones\n",
    "5) Faire passer le training_data dans le réseau \n",
    "6) Faire une fonction pour afficher les scores obtenus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xbis = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.00479897],\n",
       "        [-0.21833818],\n",
       "        [-0.01911065],\n",
       "        [ 0.25347615]],\n",
       "\n",
       "       [[-0.00916573],\n",
       "        [-0.02294864],\n",
       "        [-0.01404113],\n",
       "        [-0.04517283]],\n",
       "\n",
       "       [[-0.0096247 ],\n",
       "        [-0.21786646],\n",
       "        [-0.01494459],\n",
       "        [ 0.24304712]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.32963446],\n",
       "        [-1.30115486],\n",
       "        [ 0.14115801],\n",
       "        [ 1.354832  ]],\n",
       "\n",
       "       [[-0.35565755],\n",
       "        [-1.49773778],\n",
       "        [ 0.16825465],\n",
       "        [ 1.68813626]],\n",
       "\n",
       "       [[-0.38561231],\n",
       "        [-1.30491364],\n",
       "        [ 0.20201738],\n",
       "        [ 1.45221828]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xbis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybis = [i[1] for i in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ybis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode 2 Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 19.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 38.83 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 164.99 ticks.\n",
      "Ran 249 episodes. Solved after 149 trials ✔\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import deque\n",
    "\n",
    "class QCartPoleSolver():\n",
    "    def __init__(self, buckets=(1, 1, 6, 12,), n_episodes=1000, n_win_ticks=195, min_alpha=0.1, min_epsilon=0.1, gamma=1.0, ada_divisor=25, max_env_steps=None, quiet=False, monitor=False):\n",
    "        \n",
    "        self.buckets = buckets # down-scaling feature space to discrete range\n",
    "        \n",
    "        self.n_episodes = n_episodes # training episodes \n",
    "        \n",
    "        self.n_win_ticks = n_win_ticks # average ticks over 100 episodes required for win\n",
    "        \n",
    "        self.min_alpha = min_alpha # learning rate\n",
    "        #Alpha ( learning rate): The alpha probability determines how much the agent values newly acquired \n",
    "        #information over the older data set. If alpha was 0 the agent would learn nothing new, \n",
    "        #and at a value of1 would only make decisions based on the most recent data.\n",
    "        \n",
    "        self.min_epsilon = min_epsilon # exploration rate\n",
    "        #Epsilon (exploration rate): To avoid getting stuck in a local minimum we make our agent explore. \n",
    "        #In our case, this means choosing a random action with the probability epsilon (0 < epsilon < 1). \n",
    "        #Our new choose_action function will now look like this.\n",
    "        \n",
    "        self.gamma = gamma # discount factor\n",
    "        #Gamma (discount factor): This rate determines how to preference rewards happening sooner rather \n",
    "        #than later. A discount factor of 0 would only consider the next reward; while 1 would give all \n",
    "        #future rewards equal weight. The goal is to keep the thing upright as long as possible, \n",
    "        #so we will weight all future rewards equally.\n",
    "        \n",
    "        self.ada_divisor = ada_divisor # only for development purposes\n",
    "        \n",
    "        self.quiet = quiet\n",
    "\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, 'tmp/cartpole-1', force=True) # record results for upload\n",
    "\n",
    "        self.Q = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "\n",
    "    def discretize(self, obs):\n",
    "        upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50)]\n",
    "        lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50)]\n",
    "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "        new_obs = [int(round((self.buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        new_obs = [min(self.buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "        return tuple(new_obs)\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.Q[state])\n",
    "\n",
    "    def update_q(self, state_old, action, reward, state_new, alpha):\n",
    "        self.Q[state_old][action] += alpha * (reward + self.gamma * np.max(self.Q[state_new]) - self.Q[state_old][action])\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.min_epsilon, min(1, 1.0 - math.log10((t + 1) / self.ada_divisor)))\n",
    "\n",
    "    def get_alpha(self, t):\n",
    "        return max(self.min_alpha, min(1.0, 1.0 - math.log10((t + 1) / self.ada_divisor)))\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            current_state = self.discretize(self.env.reset())\n",
    "\n",
    "            alpha = self.get_alpha(e)\n",
    "            epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "            i = 0\n",
    "\n",
    "            while not done:\n",
    "                # self.env.render()\n",
    "                action = self.choose_action(current_state, epsilon)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize(obs)\n",
    "                self.update_q(current_state, action, reward, new_state, alpha)\n",
    "                current_state = new_state\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "        if not self.quiet: print('Did not solve after {} episodes 😞'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    solver = QCartPoleSolver()\n",
    "    solver.run()\n",
    "    # gym.upload('tmp/cartpole-1', api_key='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé méthode 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://fr.wikipedia.org/wiki/Q-learning\n",
    "La situation consiste en un agent, un ensemble d'états S {\\displaystyle S} S et d'actions A {\\displaystyle A} A. En réalisant une action a ∈ A {\\displaystyle a\\in A} a\\in A, l'agent passe d'un état à un nouvel état. L'exécution d'une action dans un état spécifique fournit à l'agent une récompense (valeur numérique). Le but de l'agent est de maximiser sa récompense totale. Cela est réalisé par apprentissage de l'action optimale pour chaque état. L'action optimale pour chaque état correspond à celle avec la plus grande récompense sur le long terme. Cette récompense est une somme pondérée de l'espérance mathématique des récompenses de chaque étape future à partir de l'état actuel. La pondération de chaque étape peut être γ Δ t {\\displaystyle \\gamma ^{\\Delta t}} {\\displaystyle \\gamma ^{\\Delta t}} où Δ t {\\displaystyle \\Delta t} \\Delta t est le délai entre l'étape actuelle et future et γ {\\displaystyle \\gamma } \\gamma un nombre entre 0 et 1 (autrement dit 0 ≤ γ ≤ 1 {\\displaystyle 0\\leq \\gamma \\leq 1} {\\displaystyle 0\\leq \\gamma \\leq 1}) appelé le facteur d'actualisation.\n",
    "\n",
    "L'algorithme calcule une fonction de valeur action-état :\n",
    "\n",
    "    Q : S × A → R {\\displaystyle Q:S\\times A\\to \\mathbb {R} } {\\displaystyle Q:S\\times A\\to \\mathbb {R} }\n",
    "\n",
    "Avant que l'apprentissage ne débute, la fonction Q est initialisée arbitrairement. Ensuite, à chaque choix d'action, l'agent observe la récompense et le nouvel état (qui dépend de l'état précédent et de l'action actuelle). Ainsi, Q {\\displaystyle Q} Q est mis à jour. Le cœur de l'algorithme est une mise à jour de la fonction de valeur. La définition de la fonction de valeur est corrigée à chaque étape de la façon suivante5 :\n",
    "\n",
    "    Q [ s , a ] := ( 1 − α ) Q [ s , a ] + α ( r + γ max a ′ Q [ s ′ , a ′ ] ) {\\displaystyle Q[s,a]:=(1-\\alpha )Q[s,a]+\\alpha \\left(r+\\gamma \\max _{a'}Q[s',a']\\right)} {\\displaystyle Q[s,a]:=(1-\\alpha )Q[s,a]+\\alpha \\left(r+\\gamma \\max _{a'}Q[s',a']\\right)}\n",
    "\n",
    "où s ′ {\\displaystyle s'} s'est le nouvel état, s {\\displaystyle s} s est l'état précédent, a {\\displaystyle a} a est l'action choisie, r {\\displaystyle r} r est la récompense reçue par l’agent, α {\\displaystyle \\alpha } \\alpha est un nombre entre 0 et 1, appelé facteur d'apprentissage, et γ {\\displaystyle \\gamma } \\gamma est le facteur d'actualisation.\n",
    "\n",
    "Un épisode de l'algorithme finit lorsque s t + 1 {\\displaystyle s_{t+1}} s_{{t+1}} est un état final. Toutefois, le Q {\\displaystyle Q} Q-learning peut aussi apprendre dans une tâche non épisodique. Si le facteur d'actualisation est plus petit que 1, la valeur action-état est finie même pour Δ t {\\displaystyle \\Delta t} \\Delta t infini.\n",
    "\n",
    "N.B. : Pour chaque état final s f {\\displaystyle s_{f}} {\\displaystyle s_{f}}, la valeur de Q ( s f , a ) {\\displaystyle Q(s_{f},a)} {\\displaystyle Q(s_{f},a)} n'est jamais mise à jour et maintient sa valeur initiale. Généralement, Q ( s f , a ) {\\displaystyle Q(s_{f},a)} {\\displaystyle Q(s_{f},a)} est initialisé à zéro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
